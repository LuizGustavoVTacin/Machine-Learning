{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc9c6d8",
   "metadata": {},
   "source": [
    "# Tópicos\n",
    "\n",
    "1. Automatic Differentiation with ```torch.autograd```\n",
    "\n",
    "\n",
    "2. Tensores, Funções e Grafos\n",
    "\n",
    "\n",
    "3. Cálculo de Gradientes\n",
    "\n",
    "4. Desabilitando o Rastreamento do Gradiente\n",
    "\n",
    "5. Mais sobre grafos computacionais\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7ffe7",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with ```torch.autograd```\n",
    "\n",
    "Em treinos de redes neurais, o algoritmo mais utilizado é o **back propagation**. Neste algoritmo, paramêtros são ajustados de acordo com o **gradiente** da função de perda.\n",
    "\n",
    "Para computar estes gradientes, PyTorch tem uma ferramente de diferenciação chamada ```torch.autograd```.\n",
    "\n",
    "Considerando uma rede de uma camada, de entrada **X**, paramêtros **w** e **b** e alguma função de perda.\n",
    "Da forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7767e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "x = torch.ones(5) #Input\n",
    "y = torch.zeros(3) #Target\n",
    "w = torch.randn(5, 3, requires_grad=True) #Weights\n",
    "b = torch.randn(3, requires_grad=True) #Bias\n",
    "z = torch.matmul(x, w) + b #Linear function\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) #Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ffb3b",
   "metadata": {},
   "source": [
    "# Tensores, Funções e Grafos\n",
    "\n",
    "O código acima define a seguinte função:\n",
    "\n",
    "![Teste](images/computational_graph.png)\n",
    "\n",
    "Nesta rede, **w**, **b** são parâmetros a serem otimizados. Então, é necessário computar gradientes da função de perda com as respectivas variáveis. Para isso, a definição ```requires_grad``` para os tensores é feita.\n",
    "\n",
    "A função aplicada aos tensores para construção dos grafos é um objeto da classe ```function```. Este objeto computa a função da direção definida em ```forward``` e computar o ```backward propagation```. Uma referência a função backward propagation está em ```grad_fn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06fc5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x0000024857CE4310>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000024857CE5120>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7706209",
   "metadata": {},
   "source": [
    "# Cálculo de Gradientes\n",
    "\n",
    "Para otimizar os pesos de uma rede neural, as derivadas da função de perda precisam ser calculadas, da forma $$ \\frac{\\partial loss}{\\partial w} $$ e $$ \\frac{\\partial loss}{\\partial b} $$\n",
    "\n",
    "O cálculo dessas derivadas é feito com ```loss.backward()``` e, en seguida, recuperar ```w.grad```e ```b.grad```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ca9fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0371, 0.0384, 0.0362],\n",
      "        [0.0371, 0.0384, 0.0362],\n",
      "        [0.0371, 0.0384, 0.0362],\n",
      "        [0.0371, 0.0384, 0.0362],\n",
      "        [0.0371, 0.0384, 0.0362]])\n",
      "tensor([0.0371, 0.0384, 0.0362])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4debeb",
   "metadata": {},
   "source": [
    "# Desabilitando o Rastreamento do Gradiente\n",
    "\n",
    "Por padrão, todos tensores ```requires_grad=True``` rastreiam o histórico computacional e fazerm o cálculo do gradiente. Há alguns casos em que não é necessário fazer isso. Por exemplo, quando um modelo é treinado e ele deve ser aplicado a apenas alguns dados de entrada, ou seja, apenas cálculos progessivos pela rede. Para interromper, faz-se ```torch.no_grad()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbe9d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac23a3",
   "metadata": {},
   "source": [
    "Desabilitar o rastreamento do gradiente quando:\n",
    "\n",
    "* Marcar parâmetros na rede como **parâmetros congelados**\n",
    "* **Acelerar os cálculos** quando a necessidade é fazer a passagem para frente, pois cálculos em tensores que não rastreiam gradientes são mais eficientes.\n",
    "\n",
    "# Mais sobre grafos computacionais\n",
    "\n",
    "O autograd mantém um registro dos dados (tensores) e de todas as operações executadas (juntamente com os novos tensores resultantes) em um grafo acíclico direcionado (DAG) composto por objetos <u>Function</u>.\n",
    "\n",
    "Neste DAG, as folhas são os tensores de entrada e as raízes são os tensores de sáida. Ao traçar este grafo das raízes às folhas, calcula-se automaticamente os gradientes usando a regra da cadeia.\n",
    "\n",
    "Mais a frente, o autograd faz:\n",
    "\n",
    "* Executa a operação solicitada para calcular um tensor resultante\n",
    "* Manter a função grandiente da operação no DAG\n",
    "\n",
    "A passagem para trás começa quando ```.backward()``` é chamado na raiz DAG.\n",
    "\n",
    "```autograd``` então:\n",
    "\n",
    "* Calcula os gradientes de cada ```.grad_fn```\n",
    "* ```.grad``` acumula-os no atributo do respectivo tensor\n",
    "* usando a regra da cadeia, propaga-se até os tensores folha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
